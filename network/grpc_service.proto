syntax = "proto3";

package llm_inference;

// Service definition for distributed LLM inference
service LLMInference {
    // Send tensor data and receive processed result
    rpc ProcessTensor(TensorRequest) returns (TensorResponse);
    
    // Health check for nodes
    rpc HealthCheck(HealthRequest) returns (HealthResponse);
    
    // Streaming interface for better performance
    rpc ProcessTensorStream(stream TensorRequest) returns (stream TensorResponse);
}

// Request message containing tensor data
message TensorRequest {
    repeated int32 shape = 1;           // Tensor dimensions
    string dtype = 2;                   // Data type (float32, float16, etc.)
    bytes data = 3;                     // Serialized tensor data
    int32 step = 4;                     // Generation step number
    string request_id = 5;              // Unique request identifier
}

// Response message containing processed tensor
message TensorResponse {
    repeated int32 shape = 1;           // Output tensor dimensions
    string dtype = 2;                   // Output data type
    bytes data = 3;                     // Serialized output tensor data
    int32 step = 4;                     // Generation step number
    string request_id = 5;              // Matching request identifier
    double processing_time = 6;         // Time taken for processing (ms)
}

// Health check messages
message HealthRequest {
    string node_id = 1;
}

message HealthResponse {
    string status = 1;                  // "healthy", "degraded", "unhealthy"
    string node_id = 2;
    double memory_usage = 3;            // Memory usage percentage
    double cpu_usage = 4;               // CPU usage percentage
}